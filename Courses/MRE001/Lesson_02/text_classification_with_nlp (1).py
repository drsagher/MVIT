# -*- coding: utf-8 -*-
"""Text_Classification_with_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hcebtZ6BKg1TlH78c2-rnZHVprtyh6jg

**Text Classification with NLP in Google Colab**

This lab demonstrates how to build and train neural networks for text classification using TensorFlow/Keras. We'll use the IMDB movie review dataset to classify sentiment (positive/negative).

**Step 1: Setup Colab Environment**
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# Check TensorFlow version and GPU
print("TensorFlow version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

# Set random seed for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

"""**Step 2: Load and Explore IMDB Dataset**"""

# Load IMDB dataset
(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)

# Decode first review
word_index = keras.datasets.imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])

print("Training samples:", len(train_data))
print("Test samples:", len(test_data))
print("\nFirst review (decoded):")
print(decoded_review[:200], "...")  # Show first 200 chars

"""**Step 3: Preprocess Text Data**"""

# Vectorize sequences (multi-hot encoding)
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

X_train = vectorize_sequences(train_data)
X_test = vectorize_sequences(test_data)

# Convert labels to float32
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

print("\nVectorized training data shape:", X_train.shape)

"""**Step 4: Build Text Classification Model**"""

def build_model():
    model = keras.Sequential([
        keras.layers.Dense(16, activation='relu', input_shape=(10000,)),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(16, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

model = build_model()
model.summary()

"""**Step 5: Train and Evaluate**"""

# Create validation set
X_val = X_train[:10000]
partial_X_train = X_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

# Callbacks
early_stopping = keras.callbacks.EarlyStopping(
    patience=3,
    restore_best_weights=True
)

# Train model
history = model.fit(
    partial_X_train,
    partial_y_train,
    epochs=20,
    batch_size=512,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""**Step 6: Using Embedding Layer (Alternative Approach)**"""

# Load raw text data
(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)

# Pad sequences to same length
maxlen = 200
X_train = keras.preprocessing.sequence.pad_sequences(
    train_data, maxlen=maxlen)
X_test = keras.preprocessing.sequence.pad_sequences(
    test_data, maxlen=maxlen)

# Build model with Embedding layer
embedding_model = keras.Sequential([
    keras.layers.Embedding(10000, 16, input_length=maxlen),
    keras.layers.GlobalAveragePooling1D(),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

embedding_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

embedding_model.summary()

# Train embedding model
history = embedding_model.fit(
    X_train, train_labels,
    epochs=10,
    batch_size=512,
    validation_split=0.2,
    verbose=1
)

"""**Step 7: Save Model and Make Predictions**"""

# Save model
model.save('imdb_classifier.h5')

# Load model
loaded_model = keras.models.load_model('imdb_classifier.h5')

# Create prediction function
def predict_sentiment(text):
    # Simple tokenization (for demo only - use proper tokenizer in production)
    words = text.lower().split()
    word_indices = [word_index.get(word, 0) for word in words]
    vectorized = vectorize_sequences([word_indices])
    prediction = loaded_model.predict(vectorized)[0][0]
    return "Positive" if prediction > 0.5 else "Negative", prediction

# Test prediction
sample_review = "This movie was absolutely fantastic and I loved every minute of it"
sentiment, confidence = predict_sentiment(sample_review)
print(f"\nReview: '{sample_review}'")
print(f"Prediction: {sentiment} (confidence: {confidence:.4f})")