# -*- coding: utf-8 -*-
"""Structured_Data_(Tabular)_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXFUROCi2HjGqOSFYBnExheSU2zVD6sL

**TensorFlow Lab: Structured Data (Tabular) Classification in Google Colab**


This lab demonstrates how to build and train a neural network for tabular data classification using TensorFlow/Keras. We'll use the Heart Disease UCI dataset to predict cardiovascular disease.

**Step 1: Setup Colab Environment**
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
import matplotlib.pyplot as plt

"""**Step 2: Load and Explore Data**"""

# Load dataset from URL
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
]
df = pd.read_csv(url, names=column_names, na_values='?')

# Clean data
df = df.dropna()
df['target'] = (df['target'] > 0).astype(int)  # Convert to binary classification

# Preview
print(f"Dataset shape: {df.shape}")
df.head()

"""**Step 3: Preprocess Data**"""

# Split features and target
X = df.drop('target', axis=1)
y = df['target']

# Identify feature types
numeric_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

# Create preprocessing pipeline
preprocessor = make_column_transformer(
    (StandardScaler(), numeric_features),
    (OneHotEncoder(), categorical_features)
)

# Split and preprocess data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")

"""**Step 4: Build the Model**"""

def create_model(input_shape):
    model = keras.Sequential([
        keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy', keras.metrics.AUC(name='auc')]
    )

    return model

model = create_model(X_train.shape[1])
model.summary()

"""**Step 5: Train the Model**"""

# Add early stopping
early_stopping = keras.callbacks.EarlyStopping(
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

"""**Step 6: Evaluate Performance**"""

# Plot training history
pd.DataFrame(history.history).plot(figsize=(10, 6))
plt.grid(True)
plt.show()

# Evaluate on test set
test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test AUC: {test_auc:.4f}")

# Make predictions
y_pred = (model.predict(X_test) > 0.5).astype(int)

# Confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**Step 7: Save and Export Model**"""

# Save model
model.save('heart_disease_model.h5')

# Load model demonstration
loaded_model = keras.models.load_model('heart_disease_model.h5')
print("\nModel loaded successfully!")